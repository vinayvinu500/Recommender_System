# -*- coding: utf-8 -*-
"""fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zp-5jUDjQIE_eVZIxFk5HtJLGnUHxja6
"""

!pip install numpy
!pip install scipy
!pip install scikit-surprise

import numpy as np
import pandas as pd
import surprise
from surprise import SVD, Dataset, Reader
from surprise.model_selection import cross_validate, GridSearchCV

import os
import random

# settings
from IPython.display import display
pd.set_option('display.max_rows', 10)
pd.set_option('display.max_columns', None)

"""## Fine Tuning Probabalistic Matrix Factorization with Suprise

"""

from google.colab import drive
drive.mount('/content/drive')

"""### Importing the dataset"""

users = pd.read_excel('/content/drive/MyDrive/Work/Datasets/BX-Users.xlsx')
books = pd.read_excel('/content/drive/MyDrive/Work/Datasets/BX-Books.xlsx')
ratings = pd.read_excel('/content/drive/MyDrive/Work/Datasets/BX-Book-Ratings.xlsx')

display(users.head(2))
display(books.head(2))
display(ratings.head(2))

# dropping duplicates
ratings.drop_duplicates(inplace=True)

# display unique records in ratings table
print('we have',ratings.shape[0], 'ratings')
print('the number of unique users we have is:', len(ratings.user_id.unique()))
print('the number of unique books we have is:', len(ratings.isbn.unique()))
print("The median user rated %d books."%ratings.user_id.value_counts().median())
print("the min rating is: %d"%ratings.rating.min(), 'The max rating is: %d'%ratings.rating.max())
ratings.head()

# Assuming 'ratings' is your DataFrame containing user IDs, item IDs, and ratings
# Example: ratings = pd.DataFrame({'userID': [...], 'itemID': [...], 'rating': [...]})

# Define a Reader and the rating_scale
reader = Reader(rating_scale=(1, 10))  # Adjust rating_scale according to your dataset

# Load the dataset from the DataFrame
data = Dataset.load_from_df(ratings[['user_id', 'isbn', 'rating']], reader)

# Define the parameter grid for SVD
param_grid = {
    'n_epochs': [5, 10, 20],  # Number of epochs
    'lr_all': [0.002, 0.005, 0.01],  # Learning rate
    'n_factors': [50, 100, 150],  # Number of factors
    'reg_all': [0.02, 0.05, 0.1]  # Regularization term
}

# Use GridSearchCV to find the best parameters for the SVD algorithm
gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)

# Fit the GridSearchCV
gs.fit(data)

# Best RMSE score
print('Best RMSE:', gs.best_score['rmse'])

# Combination of parameters that gave the best RMSE score
print('Best parameters:', gs.best_params['rmse'])

# Use the best parameters to create a new SVD model
best_svd = SVD(**gs.best_params['rmse'])

# You can now train this model on the full dataset and use it for predictions
# For example, using cross-validation to evaluate its performance:
cross_validate(best_svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Assuming 'ratings' DataFrame contains columns ['user_id', 'isbn', 'rating']
# Select a random user
random_user = random.choice(ratings['user_id'].unique())

# Select a random book
random_book = random.choice(ratings['isbn'].unique())

print(f"Random User ID: {random_user}")
print(f"Random Book ID: {random_book}")

# Assuming best_svd is your trained SVD model with the best parameters
pred = best_svd.predict(uid=str(random_user), iid=str(random_book))

# Print the predicted rating
print(f"Predicted rating for user {pred.uid} on book {pred.iid} is: {pred.est}")

# Function to get recommendations for a user
def get_recommendations(user_id, model, n=10):
    # Convert the user's ratings to an anti-testset (books the user hasn't rated)
    testset = [[user_id, isbn, 0] for isbn in ratings['isbn'].unique() if isbn not in ratings[ratings['user_id'] == user_id]['isbn']]
    predictions = model.test(testset)

    # Get the top N recommendations
    top_n_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:n]

    recommended_books = [(pred.iid, pred.est) for pred in top_n_predictions]
    return recommended_books

# Example usage
user_id = '203092'
n = 5
recommended_books = get_recommendations(user_id, best_svd, n=n)
print(f"Top {n} recommended books for user {user_id}:")
for isbn, rating_pred in recommended_books:
    print(f"ISBN: {isbn}, Predicted Rating: {rating_pred}")
    # print(f"{books[books.isbn == isbn].title}")

# Dump the model
import pickle
path = "/content/drive/MyDrive/Work/Datasets/"

# saving the model
file_path = os.path.join(path, "model.pkl")
with open(file_path, 'wb') as file:
  pickle.dump(best_svd, file)

print(f"Model saved to {file_path}")

"""***Important Notes***
- <b>Model Compatibility</b>: When you load the model in a different environment or after updating libraries, ensure the environment is compatible with the one used for training the model. This includes having the same versions of Surprise, scikit-learn, numpy, and other relevant libraries.
- <b>Security</b>: Be cautious when loading pickle files from untrusted sources, as they can execute arbitrary code.
"""

# Load the model
with open(file_path, 'rb') as file:
  model = pickle.load(file)

# Function to get recommendations for a user
def get_recommendations(user_id, model, n=10):
    # Convert the user's ratings to an anti-testset (books the user hasn't rated)
    testset = [[user_id, isbn, 0] for isbn in ratings['isbn'].unique() if isbn not in ratings[ratings['user_id'] == user_id]['isbn']]
    predictions = model.test(testset)

    # Get the top N recommendations
    top_n_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)[:n]

    recommended_books = [(pred.iid, pred.est) for pred in top_n_predictions]
    return recommended_books

# Example usage
user_id = '203092'
n = 5
recommended_books = get_recommendations(user_id, model, n=n)
print(f"Top {n} recommended books for user {user_id}:")
for isbn, rating_pred in recommended_books:
    print(f"ISBN: {isbn}, Predicted Rating: {rating_pred}")
    # print(f"{books[books.isbn == isbn].title}")